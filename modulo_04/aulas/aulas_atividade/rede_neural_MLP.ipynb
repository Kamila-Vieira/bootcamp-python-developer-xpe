{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_RW9DCWPWfUU",
        "outputId": "04a61212-e480-451f-8029-2859623608df"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-288bec4b-6529-4884-9e4f-c30dccf6e108\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clientid</th>\n",
              "      <th>income</th>\n",
              "      <th>age</th>\n",
              "      <th>loan</th>\n",
              "      <th>default</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>66155.925095</td>\n",
              "      <td>59.017015</td>\n",
              "      <td>8106.532131</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>34415.153966</td>\n",
              "      <td>48.117153</td>\n",
              "      <td>6564.745018</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>57317.170063</td>\n",
              "      <td>63.108049</td>\n",
              "      <td>8020.953296</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>42709.534201</td>\n",
              "      <td>45.751972</td>\n",
              "      <td>6103.642260</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>66952.688845</td>\n",
              "      <td>18.584336</td>\n",
              "      <td>8770.099235</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-288bec4b-6529-4884-9e4f-c30dccf6e108')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-288bec4b-6529-4884-9e4f-c30dccf6e108 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-288bec4b-6529-4884-9e4f-c30dccf6e108');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   clientid        income        age         loan  default\n",
              "0         1  66155.925095  59.017015  8106.532131        0\n",
              "1         2  34415.153966  48.117153  6564.745018        0\n",
              "2         3  57317.170063  63.108049  8020.953296        0\n",
              "3         4  42709.534201  45.751972  6103.642260        0\n",
              "4         5  66952.688845  18.584336  8770.099235        1"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('credit-data.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j8v9i42P8JIo"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKaxg4WWg4gN",
        "outputId": "f73549c1-b821-4af5-84d2-5cbb852512b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1997, 5)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xU8XDJvOg8lN"
      },
      "outputs": [],
      "source": [
        "x_credit = df[['income', 'age', 'loan']]\n",
        "y_credit = df[['default']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mzRNoN6AaE3p"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VhFwvOfMaKtm"
      },
      "outputs": [],
      "source": [
        "standard_scaler = StandardScaler()\n",
        "\n",
        "x_credit = standard_scaler.fit_transform(x_credit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjEzvBXeaW94",
        "outputId": "441c9805-a1d1-4880-d77f-e8ac1ecd7e71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1.45389775,  1.33686061,  1.20190707],\n",
              "       [-0.76239757,  0.53663921,  0.69574418],\n",
              "       [ 0.8367328 ,  1.63720692,  1.17381186],\n",
              "       ...,\n",
              "       [-0.07139   , -0.93901609,  0.35367319],\n",
              "       [-0.11017022,  1.7006195 , -0.92670314],\n",
              "       [ 1.68296904,  1.12656872,  0.96300639]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_credit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "68iFlR5oaZNy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "n6CgI_TXad5C"
      },
      "outputs": [],
      "source": [
        "x_credit_train, x_credit_test, y_credit_train, y_credit_test = train_test_split(x_credit, y_credit, test_size=0.25, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsC9bBfNa0LF",
        "outputId": "22b1798f-2582-4a04-fcb7-ea172f53d897"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1497, 3)\n",
            "(500, 3)\n",
            "(1497, 1)\n",
            "(500, 1)\n"
          ]
        }
      ],
      "source": [
        "print(x_credit_train.shape)\n",
        "print(x_credit_test.shape)\n",
        "print(y_credit_train.shape)\n",
        "print(y_credit_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IPIH5sgJbTmS"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "UlFsFltI7nMS"
      },
      "outputs": [],
      "source": [
        "rede_neural_credit = MLPClassifier(verbose = True, max_iter=1000, tol=0.0000100,\n",
        "                                   hidden_layer_sizes=(20,100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQw0YrhQ72sT",
        "outputId": "0e1fb338-4be8-47df-91d2-810211841505"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.64046841\n",
            "Iteration 2, loss = 0.52957613\n",
            "Iteration 3, loss = 0.45314357\n",
            "Iteration 4, loss = 0.39965336\n",
            "Iteration 5, loss = 0.36253057\n",
            "Iteration 6, loss = 0.33300662\n",
            "Iteration 7, loss = 0.30633303\n",
            "Iteration 8, loss = 0.28126583\n",
            "Iteration 9, loss = 0.25842177\n",
            "Iteration 10, loss = 0.23687584\n",
            "Iteration 11, loss = 0.21685859\n",
            "Iteration 12, loss = 0.19880783\n",
            "Iteration 13, loss = 0.18221196\n",
            "Iteration 14, loss = 0.16742550\n",
            "Iteration 15, loss = 0.15417211\n",
            "Iteration 16, loss = 0.14252281\n",
            "Iteration 17, loss = 0.13215436\n",
            "Iteration 18, loss = 0.12284864\n",
            "Iteration 19, loss = 0.11444288\n",
            "Iteration 20, loss = 0.10663153\n",
            "Iteration 21, loss = 0.09967299\n",
            "Iteration 22, loss = 0.09345786\n",
            "Iteration 23, loss = 0.08777295\n",
            "Iteration 24, loss = 0.08264537\n",
            "Iteration 25, loss = 0.07830010\n",
            "Iteration 26, loss = 0.07443340\n",
            "Iteration 27, loss = 0.07070160\n",
            "Iteration 28, loss = 0.06747757\n",
            "Iteration 29, loss = 0.06437654\n",
            "Iteration 30, loss = 0.06160936\n",
            "Iteration 31, loss = 0.05895780\n",
            "Iteration 32, loss = 0.05680211\n",
            "Iteration 33, loss = 0.05443990\n",
            "Iteration 34, loss = 0.05276605\n",
            "Iteration 35, loss = 0.05104134\n",
            "Iteration 36, loss = 0.04905025\n",
            "Iteration 37, loss = 0.04747037\n",
            "Iteration 38, loss = 0.04611689\n",
            "Iteration 39, loss = 0.04497125\n",
            "Iteration 40, loss = 0.04354716\n",
            "Iteration 41, loss = 0.04246429\n",
            "Iteration 42, loss = 0.04125610\n",
            "Iteration 43, loss = 0.04019965\n",
            "Iteration 44, loss = 0.03923119\n",
            "Iteration 45, loss = 0.03825684\n",
            "Iteration 46, loss = 0.03742676\n",
            "Iteration 47, loss = 0.03654510\n",
            "Iteration 48, loss = 0.03594642\n",
            "Iteration 49, loss = 0.03484172\n",
            "Iteration 50, loss = 0.03450704\n",
            "Iteration 51, loss = 0.03350773\n",
            "Iteration 52, loss = 0.03297683\n",
            "Iteration 53, loss = 0.03231073\n",
            "Iteration 54, loss = 0.03176243\n",
            "Iteration 55, loss = 0.03110195\n",
            "Iteration 56, loss = 0.03066424\n",
            "Iteration 57, loss = 0.03049896\n",
            "Iteration 58, loss = 0.02953397\n",
            "Iteration 59, loss = 0.02913173\n",
            "Iteration 60, loss = 0.02850497\n",
            "Iteration 61, loss = 0.02803638\n",
            "Iteration 62, loss = 0.02755115\n",
            "Iteration 63, loss = 0.02735164\n",
            "Iteration 64, loss = 0.02692751\n",
            "Iteration 65, loss = 0.02666643\n",
            "Iteration 66, loss = 0.02579010\n",
            "Iteration 67, loss = 0.02568173\n",
            "Iteration 68, loss = 0.02510390\n",
            "Iteration 69, loss = 0.02491481\n",
            "Iteration 70, loss = 0.02435494\n",
            "Iteration 71, loss = 0.02413507\n",
            "Iteration 72, loss = 0.02391627\n",
            "Iteration 73, loss = 0.02358948\n",
            "Iteration 74, loss = 0.02301061\n",
            "Iteration 75, loss = 0.02253124\n",
            "Iteration 76, loss = 0.02221897\n",
            "Iteration 77, loss = 0.02189932\n",
            "Iteration 78, loss = 0.02160811\n",
            "Iteration 79, loss = 0.02153377\n",
            "Iteration 80, loss = 0.02120570\n",
            "Iteration 81, loss = 0.02079548\n",
            "Iteration 82, loss = 0.02071531\n",
            "Iteration 83, loss = 0.02036052\n",
            "Iteration 84, loss = 0.02002961\n",
            "Iteration 85, loss = 0.01981814\n",
            "Iteration 86, loss = 0.01953796\n",
            "Iteration 87, loss = 0.01933974\n",
            "Iteration 88, loss = 0.01907453\n",
            "Iteration 89, loss = 0.01892124\n",
            "Iteration 90, loss = 0.01877611\n",
            "Iteration 91, loss = 0.01870381\n",
            "Iteration 92, loss = 0.01828565\n",
            "Iteration 93, loss = 0.01799925\n",
            "Iteration 94, loss = 0.01798941\n",
            "Iteration 95, loss = 0.01765367\n",
            "Iteration 96, loss = 0.01740347\n",
            "Iteration 97, loss = 0.01719608\n",
            "Iteration 98, loss = 0.01707828\n",
            "Iteration 99, loss = 0.01689027\n",
            "Iteration 100, loss = 0.01681554\n",
            "Iteration 101, loss = 0.01650399\n",
            "Iteration 102, loss = 0.01626108\n",
            "Iteration 103, loss = 0.01618085\n",
            "Iteration 104, loss = 0.01600310\n",
            "Iteration 105, loss = 0.01585196\n",
            "Iteration 106, loss = 0.01604454\n",
            "Iteration 107, loss = 0.01557853\n",
            "Iteration 108, loss = 0.01541820\n",
            "Iteration 109, loss = 0.01525397\n",
            "Iteration 110, loss = 0.01514575\n",
            "Iteration 111, loss = 0.01507183\n",
            "Iteration 112, loss = 0.01491346\n",
            "Iteration 113, loss = 0.01480682\n",
            "Iteration 114, loss = 0.01448601\n",
            "Iteration 115, loss = 0.01478192\n",
            "Iteration 116, loss = 0.01497975\n",
            "Iteration 117, loss = 0.01384595\n",
            "Iteration 118, loss = 0.01503027\n",
            "Iteration 119, loss = 0.01419495\n",
            "Iteration 120, loss = 0.01400035\n",
            "Iteration 121, loss = 0.01354087\n",
            "Iteration 122, loss = 0.01347307\n",
            "Iteration 123, loss = 0.01330874\n",
            "Iteration 124, loss = 0.01331899\n",
            "Iteration 125, loss = 0.01355351\n",
            "Iteration 126, loss = 0.01319851\n",
            "Iteration 127, loss = 0.01285544\n",
            "Iteration 128, loss = 0.01275483\n",
            "Iteration 129, loss = 0.01284941\n",
            "Iteration 130, loss = 0.01256668\n",
            "Iteration 131, loss = 0.01252752\n",
            "Iteration 132, loss = 0.01230552\n",
            "Iteration 133, loss = 0.01223234\n",
            "Iteration 134, loss = 0.01244911\n",
            "Iteration 135, loss = 0.01192774\n",
            "Iteration 136, loss = 0.01202365\n",
            "Iteration 137, loss = 0.01176330\n",
            "Iteration 138, loss = 0.01178484\n",
            "Iteration 139, loss = 0.01167410\n",
            "Iteration 140, loss = 0.01169852\n",
            "Iteration 141, loss = 0.01143507\n",
            "Iteration 142, loss = 0.01139852\n",
            "Iteration 143, loss = 0.01158128\n",
            "Iteration 144, loss = 0.01118050\n",
            "Iteration 145, loss = 0.01109082\n",
            "Iteration 146, loss = 0.01106993\n",
            "Iteration 147, loss = 0.01094365\n",
            "Iteration 148, loss = 0.01076474\n",
            "Iteration 149, loss = 0.01079067\n",
            "Iteration 150, loss = 0.01065177\n",
            "Iteration 151, loss = 0.01063083\n",
            "Iteration 152, loss = 0.01035816\n",
            "Iteration 153, loss = 0.01038622\n",
            "Iteration 154, loss = 0.01029807\n",
            "Iteration 155, loss = 0.01029084\n",
            "Iteration 156, loss = 0.01056598\n",
            "Iteration 157, loss = 0.01000694\n",
            "Iteration 158, loss = 0.01010100\n",
            "Iteration 159, loss = 0.01016453\n",
            "Iteration 160, loss = 0.00987784\n",
            "Iteration 161, loss = 0.00986971\n",
            "Iteration 162, loss = 0.00980984\n",
            "Iteration 163, loss = 0.00958093\n",
            "Iteration 164, loss = 0.00963701\n",
            "Iteration 165, loss = 0.00944347\n",
            "Iteration 166, loss = 0.00927257\n",
            "Iteration 167, loss = 0.00922835\n",
            "Iteration 168, loss = 0.00919548\n",
            "Iteration 169, loss = 0.00910546\n",
            "Iteration 170, loss = 0.00928782\n",
            "Iteration 171, loss = 0.00907960\n",
            "Iteration 172, loss = 0.00903075\n",
            "Iteration 173, loss = 0.00888270\n",
            "Iteration 174, loss = 0.00907038\n",
            "Iteration 175, loss = 0.00872939\n",
            "Iteration 176, loss = 0.00898219\n",
            "Iteration 177, loss = 0.00867293\n",
            "Iteration 178, loss = 0.00858315\n",
            "Iteration 179, loss = 0.00854641\n",
            "Iteration 180, loss = 0.00840763\n",
            "Iteration 181, loss = 0.00843137\n",
            "Iteration 182, loss = 0.00861518\n",
            "Iteration 183, loss = 0.00840376\n",
            "Iteration 184, loss = 0.00835781\n",
            "Iteration 185, loss = 0.00816888\n",
            "Iteration 186, loss = 0.00806018\n",
            "Iteration 187, loss = 0.00810232\n",
            "Iteration 188, loss = 0.00793823\n",
            "Iteration 189, loss = 0.00813446\n",
            "Iteration 190, loss = 0.00801053\n",
            "Iteration 191, loss = 0.00775081\n",
            "Iteration 192, loss = 0.00800282\n",
            "Iteration 193, loss = 0.00781434\n",
            "Iteration 194, loss = 0.00770659\n",
            "Iteration 195, loss = 0.00770142\n",
            "Iteration 196, loss = 0.00753560\n",
            "Iteration 197, loss = 0.00763774\n",
            "Iteration 198, loss = 0.00743328\n",
            "Iteration 199, loss = 0.00736995\n",
            "Iteration 200, loss = 0.00732753\n",
            "Iteration 201, loss = 0.00735359\n",
            "Iteration 202, loss = 0.00737801\n",
            "Iteration 203, loss = 0.00724819\n",
            "Iteration 204, loss = 0.00732993\n",
            "Iteration 205, loss = 0.00706813\n",
            "Iteration 206, loss = 0.00721319\n",
            "Iteration 207, loss = 0.00703253\n",
            "Iteration 208, loss = 0.00706356\n",
            "Iteration 209, loss = 0.00725517\n",
            "Iteration 210, loss = 0.00697176\n",
            "Iteration 211, loss = 0.00689391\n",
            "Iteration 212, loss = 0.00685552\n",
            "Iteration 213, loss = 0.00671877\n",
            "Iteration 214, loss = 0.00671573\n",
            "Iteration 215, loss = 0.00664075\n",
            "Iteration 216, loss = 0.00658740\n",
            "Iteration 217, loss = 0.00651473\n",
            "Iteration 218, loss = 0.00652201\n",
            "Iteration 219, loss = 0.00647006\n",
            "Iteration 220, loss = 0.00655682\n",
            "Iteration 221, loss = 0.00630054\n",
            "Iteration 222, loss = 0.00632928\n",
            "Iteration 223, loss = 0.00629266\n",
            "Iteration 224, loss = 0.00627459\n",
            "Iteration 225, loss = 0.00624745\n",
            "Iteration 226, loss = 0.00629965\n",
            "Iteration 227, loss = 0.00619089\n",
            "Iteration 228, loss = 0.00612716\n",
            "Iteration 229, loss = 0.00608419\n",
            "Iteration 230, loss = 0.00593875\n",
            "Iteration 231, loss = 0.00612536\n",
            "Iteration 232, loss = 0.00591339\n",
            "Iteration 233, loss = 0.00602901\n",
            "Iteration 234, loss = 0.00580133\n",
            "Iteration 235, loss = 0.00631707\n",
            "Iteration 236, loss = 0.00639611\n",
            "Iteration 237, loss = 0.00604516\n",
            "Iteration 238, loss = 0.00581794\n",
            "Iteration 239, loss = 0.00575841\n",
            "Iteration 240, loss = 0.00562971\n",
            "Iteration 241, loss = 0.00558194\n",
            "Iteration 242, loss = 0.00577294\n",
            "Iteration 243, loss = 0.00557556\n",
            "Iteration 244, loss = 0.00563165\n",
            "Iteration 245, loss = 0.00544651\n",
            "Iteration 246, loss = 0.00549091\n",
            "Iteration 247, loss = 0.00582676\n",
            "Iteration 248, loss = 0.00543801\n",
            "Iteration 249, loss = 0.00560325\n",
            "Iteration 250, loss = 0.00539572\n",
            "Iteration 251, loss = 0.00527535\n",
            "Iteration 252, loss = 0.00535737\n",
            "Iteration 253, loss = 0.00535437\n",
            "Iteration 254, loss = 0.00528005\n",
            "Iteration 255, loss = 0.00529585\n",
            "Iteration 256, loss = 0.00511419\n",
            "Iteration 257, loss = 0.00517447\n",
            "Iteration 258, loss = 0.00508478\n",
            "Iteration 259, loss = 0.00502878\n",
            "Iteration 260, loss = 0.00540803\n",
            "Iteration 261, loss = 0.00518450\n",
            "Iteration 262, loss = 0.00518042\n",
            "Iteration 263, loss = 0.00513752\n",
            "Iteration 264, loss = 0.00495226\n",
            "Iteration 265, loss = 0.00485049\n",
            "Iteration 266, loss = 0.00488559\n",
            "Iteration 267, loss = 0.00496418\n",
            "Iteration 268, loss = 0.00479360\n",
            "Iteration 269, loss = 0.00468781\n",
            "Iteration 270, loss = 0.00477643\n",
            "Iteration 271, loss = 0.00465614\n",
            "Iteration 272, loss = 0.00532565\n",
            "Iteration 273, loss = 0.00460782\n",
            "Iteration 274, loss = 0.00511034\n",
            "Iteration 275, loss = 0.00459657\n",
            "Iteration 276, loss = 0.00502242\n",
            "Iteration 277, loss = 0.00452773\n",
            "Iteration 278, loss = 0.00473894\n",
            "Iteration 279, loss = 0.00459270\n",
            "Iteration 280, loss = 0.00455597\n",
            "Iteration 281, loss = 0.00465685\n",
            "Iteration 282, loss = 0.00438741\n",
            "Iteration 283, loss = 0.00447560\n",
            "Iteration 284, loss = 0.00450373\n",
            "Iteration 285, loss = 0.00447009\n",
            "Iteration 286, loss = 0.00435986\n",
            "Iteration 287, loss = 0.00427089\n",
            "Iteration 288, loss = 0.00428885\n",
            "Iteration 289, loss = 0.00429280\n",
            "Iteration 290, loss = 0.00425905\n",
            "Iteration 291, loss = 0.00429026\n",
            "Iteration 292, loss = 0.00426169\n",
            "Iteration 293, loss = 0.00461597\n",
            "Iteration 294, loss = 0.00398674\n",
            "Iteration 295, loss = 0.00422743\n",
            "Iteration 296, loss = 0.00406156\n",
            "Iteration 297, loss = 0.00414004\n",
            "Iteration 298, loss = 0.00407996\n",
            "Iteration 299, loss = 0.00414761\n",
            "Iteration 300, loss = 0.00415130\n",
            "Iteration 301, loss = 0.00422765\n",
            "Iteration 302, loss = 0.00409239\n",
            "Iteration 303, loss = 0.00395607\n",
            "Iteration 304, loss = 0.00399925\n",
            "Iteration 305, loss = 0.00402291\n",
            "Iteration 306, loss = 0.00409321\n",
            "Iteration 307, loss = 0.00388235\n",
            "Iteration 308, loss = 0.00390564\n",
            "Iteration 309, loss = 0.00397361\n",
            "Iteration 310, loss = 0.00378384\n",
            "Iteration 311, loss = 0.00387218\n",
            "Iteration 312, loss = 0.00392978\n",
            "Iteration 313, loss = 0.00377345\n",
            "Iteration 314, loss = 0.00376081\n",
            "Iteration 315, loss = 0.00387335\n",
            "Iteration 316, loss = 0.00378269\n",
            "Iteration 317, loss = 0.00363447\n",
            "Iteration 318, loss = 0.00388210\n",
            "Iteration 319, loss = 0.00366897\n",
            "Iteration 320, loss = 0.00369692\n",
            "Iteration 321, loss = 0.00364400\n",
            "Iteration 322, loss = 0.00365561\n",
            "Iteration 323, loss = 0.00364284\n",
            "Iteration 324, loss = 0.00346233\n",
            "Iteration 325, loss = 0.00358121\n",
            "Iteration 326, loss = 0.00362926\n",
            "Iteration 327, loss = 0.00356365\n",
            "Iteration 328, loss = 0.00353479\n",
            "Iteration 329, loss = 0.00341370\n",
            "Iteration 330, loss = 0.00342327\n",
            "Iteration 331, loss = 0.00330693\n",
            "Iteration 332, loss = 0.00346683\n",
            "Iteration 333, loss = 0.00332146\n",
            "Iteration 334, loss = 0.00332900\n",
            "Iteration 335, loss = 0.00342404\n",
            "Iteration 336, loss = 0.00330656\n",
            "Iteration 337, loss = 0.00336330\n",
            "Iteration 338, loss = 0.00327362\n",
            "Iteration 339, loss = 0.00329744\n",
            "Iteration 340, loss = 0.00334737\n",
            "Iteration 341, loss = 0.00324142\n",
            "Iteration 342, loss = 0.00342776\n",
            "Iteration 343, loss = 0.00314844\n",
            "Iteration 344, loss = 0.00327105\n",
            "Iteration 345, loss = 0.00317391\n",
            "Iteration 346, loss = 0.00317213\n",
            "Iteration 347, loss = 0.00315170\n",
            "Iteration 348, loss = 0.00310387\n",
            "Iteration 349, loss = 0.00306564\n",
            "Iteration 350, loss = 0.00306882\n",
            "Iteration 351, loss = 0.00301967\n",
            "Iteration 352, loss = 0.00310240\n",
            "Iteration 353, loss = 0.00304618\n",
            "Iteration 354, loss = 0.00292322\n",
            "Iteration 355, loss = 0.00314643\n",
            "Iteration 356, loss = 0.00297120\n",
            "Iteration 357, loss = 0.00293832\n",
            "Iteration 358, loss = 0.00296169\n",
            "Iteration 359, loss = 0.00292480\n",
            "Iteration 360, loss = 0.00284793\n",
            "Iteration 361, loss = 0.00283901\n",
            "Iteration 362, loss = 0.00291932\n",
            "Iteration 363, loss = 0.00284453\n",
            "Iteration 364, loss = 0.00291046\n",
            "Iteration 365, loss = 0.00288418\n",
            "Iteration 366, loss = 0.00284565\n",
            "Iteration 367, loss = 0.00291420\n",
            "Iteration 368, loss = 0.00278058\n",
            "Iteration 369, loss = 0.00275662\n",
            "Iteration 370, loss = 0.00282714\n",
            "Iteration 371, loss = 0.00278139\n",
            "Iteration 372, loss = 0.00278038\n",
            "Iteration 373, loss = 0.00279325\n",
            "Iteration 374, loss = 0.00266559\n",
            "Iteration 375, loss = 0.00266375\n",
            "Iteration 376, loss = 0.00266363\n",
            "Iteration 377, loss = 0.00268628\n",
            "Iteration 378, loss = 0.00266300\n",
            "Iteration 379, loss = 0.00265989\n",
            "Iteration 380, loss = 0.00265628\n",
            "Iteration 381, loss = 0.00264806\n",
            "Iteration 382, loss = 0.00256311\n",
            "Iteration 383, loss = 0.00263537\n",
            "Iteration 384, loss = 0.00261925\n",
            "Iteration 385, loss = 0.00278241\n",
            "Iteration 386, loss = 0.00269380\n",
            "Iteration 387, loss = 0.00267940\n",
            "Iteration 388, loss = 0.00262006\n",
            "Iteration 389, loss = 0.00260816\n",
            "Iteration 390, loss = 0.00247859\n",
            "Iteration 391, loss = 0.00253567\n",
            "Iteration 392, loss = 0.00253029\n",
            "Iteration 393, loss = 0.00238762\n",
            "Iteration 394, loss = 0.00244894\n",
            "Iteration 395, loss = 0.00242118\n",
            "Iteration 396, loss = 0.00246486\n",
            "Iteration 397, loss = 0.00256695\n",
            "Iteration 398, loss = 0.00346718\n",
            "Iteration 399, loss = 0.00258767\n",
            "Iteration 400, loss = 0.00254378\n",
            "Iteration 401, loss = 0.00236890\n",
            "Iteration 402, loss = 0.00235940\n",
            "Iteration 403, loss = 0.00247135\n",
            "Iteration 404, loss = 0.00229415\n",
            "Iteration 405, loss = 0.00251468\n",
            "Iteration 406, loss = 0.00234086\n",
            "Iteration 407, loss = 0.00233098\n",
            "Iteration 408, loss = 0.00239760\n",
            "Iteration 409, loss = 0.00223550\n",
            "Iteration 410, loss = 0.00228421\n",
            "Iteration 411, loss = 0.00234808\n",
            "Iteration 412, loss = 0.00238316\n",
            "Iteration 413, loss = 0.00228336\n",
            "Iteration 414, loss = 0.00239638\n",
            "Iteration 415, loss = 0.00222988\n",
            "Iteration 416, loss = 0.00216134\n",
            "Iteration 417, loss = 0.00247279\n",
            "Iteration 418, loss = 0.00217293\n",
            "Iteration 419, loss = 0.00222374\n",
            "Iteration 420, loss = 0.00219698\n",
            "Iteration 421, loss = 0.00225623\n",
            "Iteration 422, loss = 0.00215014\n",
            "Iteration 423, loss = 0.00206817\n",
            "Iteration 424, loss = 0.00216530\n",
            "Iteration 425, loss = 0.00216196\n",
            "Iteration 426, loss = 0.00209923\n",
            "Iteration 427, loss = 0.00220588\n",
            "Iteration 428, loss = 0.00212111\n",
            "Iteration 429, loss = 0.00213830\n",
            "Iteration 430, loss = 0.00213411\n",
            "Iteration 431, loss = 0.00229145\n",
            "Iteration 432, loss = 0.00218460\n",
            "Iteration 433, loss = 0.00217760\n",
            "Iteration 434, loss = 0.00205078\n",
            "Iteration 435, loss = 0.00216825\n",
            "Iteration 436, loss = 0.00192966\n",
            "Iteration 437, loss = 0.00218094\n",
            "Iteration 438, loss = 0.00190786\n",
            "Iteration 439, loss = 0.00194341\n",
            "Iteration 440, loss = 0.00191529\n",
            "Iteration 441, loss = 0.00194175\n",
            "Iteration 442, loss = 0.00202916\n",
            "Iteration 443, loss = 0.00198600\n",
            "Iteration 444, loss = 0.00209037\n",
            "Iteration 445, loss = 0.00185472\n",
            "Iteration 446, loss = 0.00185237\n",
            "Iteration 447, loss = 0.00198540\n",
            "Iteration 448, loss = 0.00184367\n",
            "Iteration 449, loss = 0.00197031\n",
            "Iteration 450, loss = 0.00182119\n",
            "Iteration 451, loss = 0.00188006\n",
            "Iteration 452, loss = 0.00184561\n",
            "Iteration 453, loss = 0.00189252\n",
            "Iteration 454, loss = 0.00180672\n",
            "Iteration 455, loss = 0.00179692\n",
            "Iteration 456, loss = 0.00183393\n",
            "Iteration 457, loss = 0.00198983\n",
            "Iteration 458, loss = 0.00178243\n",
            "Iteration 459, loss = 0.00208379\n",
            "Iteration 460, loss = 0.00199969\n",
            "Iteration 461, loss = 0.00182586\n",
            "Iteration 462, loss = 0.00195559\n",
            "Iteration 463, loss = 0.00167453\n",
            "Iteration 464, loss = 0.00186090\n",
            "Iteration 465, loss = 0.00183935\n",
            "Iteration 466, loss = 0.00178193\n",
            "Iteration 467, loss = 0.00168225\n",
            "Iteration 468, loss = 0.00173272\n",
            "Iteration 469, loss = 0.00174760\n",
            "Iteration 470, loss = 0.00174667\n",
            "Iteration 471, loss = 0.00168175\n",
            "Iteration 472, loss = 0.00165925\n",
            "Iteration 473, loss = 0.00164566\n",
            "Iteration 474, loss = 0.00172113\n",
            "Iteration 475, loss = 0.00164738\n",
            "Iteration 476, loss = 0.00172633\n",
            "Iteration 477, loss = 0.00160303\n",
            "Iteration 478, loss = 0.00166574\n",
            "Iteration 479, loss = 0.00161402\n",
            "Iteration 480, loss = 0.00161907\n",
            "Iteration 481, loss = 0.00161133\n",
            "Iteration 482, loss = 0.00161896\n",
            "Iteration 483, loss = 0.00184241\n",
            "Iteration 484, loss = 0.00176225\n",
            "Iteration 485, loss = 0.00172650\n",
            "Iteration 486, loss = 0.00154428\n",
            "Iteration 487, loss = 0.00158368\n",
            "Iteration 488, loss = 0.00150709\n",
            "Iteration 489, loss = 0.00160298\n",
            "Iteration 490, loss = 0.00160795\n",
            "Iteration 491, loss = 0.00154855\n",
            "Iteration 492, loss = 0.00156001\n",
            "Iteration 493, loss = 0.00170800\n",
            "Iteration 494, loss = 0.00145281\n",
            "Iteration 495, loss = 0.00155167\n",
            "Iteration 496, loss = 0.00151815\n",
            "Iteration 497, loss = 0.00161125\n",
            "Iteration 498, loss = 0.00145158\n",
            "Iteration 499, loss = 0.00166518\n",
            "Iteration 500, loss = 0.00158563\n",
            "Iteration 501, loss = 0.00151935\n",
            "Iteration 502, loss = 0.00146882\n",
            "Iteration 503, loss = 0.00147684\n",
            "Iteration 504, loss = 0.00141667\n",
            "Iteration 505, loss = 0.00147369\n",
            "Iteration 506, loss = 0.00172757\n",
            "Iteration 507, loss = 0.00137040\n",
            "Iteration 508, loss = 0.00153341\n",
            "Iteration 509, loss = 0.00149017\n",
            "Iteration 510, loss = 0.00143203\n",
            "Iteration 511, loss = 0.00141931\n",
            "Iteration 512, loss = 0.00139430\n",
            "Iteration 513, loss = 0.00154907\n",
            "Iteration 514, loss = 0.00154728\n",
            "Iteration 515, loss = 0.00147070\n",
            "Iteration 516, loss = 0.00155820\n",
            "Iteration 517, loss = 0.00132554\n",
            "Iteration 518, loss = 0.00143480\n",
            "Iteration 519, loss = 0.00135375\n",
            "Iteration 520, loss = 0.00135375\n",
            "Iteration 521, loss = 0.00137667\n",
            "Iteration 522, loss = 0.00141307\n",
            "Iteration 523, loss = 0.00130141\n",
            "Iteration 524, loss = 0.00131280\n",
            "Iteration 525, loss = 0.00135196\n",
            "Iteration 526, loss = 0.00146872\n",
            "Iteration 527, loss = 0.00141971\n",
            "Iteration 528, loss = 0.00127853\n",
            "Iteration 529, loss = 0.00131473\n",
            "Iteration 530, loss = 0.00130286\n",
            "Iteration 531, loss = 0.00129409\n",
            "Iteration 532, loss = 0.00141453\n",
            "Iteration 533, loss = 0.00145258\n",
            "Iteration 534, loss = 0.00131678\n",
            "Iteration 535, loss = 0.00125209\n",
            "Iteration 536, loss = 0.00126833\n",
            "Iteration 537, loss = 0.00129502\n",
            "Iteration 538, loss = 0.00120573\n",
            "Iteration 539, loss = 0.00128603\n",
            "Iteration 540, loss = 0.00125006\n",
            "Iteration 541, loss = 0.00126106\n",
            "Iteration 542, loss = 0.00125980\n",
            "Iteration 543, loss = 0.00127682\n",
            "Iteration 544, loss = 0.00119402\n",
            "Iteration 545, loss = 0.00121448\n",
            "Iteration 546, loss = 0.00117484\n",
            "Iteration 547, loss = 0.00121487\n",
            "Iteration 548, loss = 0.00121357\n",
            "Iteration 549, loss = 0.00117203\n",
            "Iteration 550, loss = 0.00115781\n",
            "Iteration 551, loss = 0.00119038\n",
            "Iteration 552, loss = 0.00118136\n",
            "Iteration 553, loss = 0.00124557\n",
            "Iteration 554, loss = 0.00109097\n",
            "Iteration 555, loss = 0.00138369\n",
            "Iteration 556, loss = 0.00125501\n",
            "Iteration 557, loss = 0.00120317\n",
            "Iteration 558, loss = 0.00123755\n",
            "Iteration 559, loss = 0.00118847\n",
            "Iteration 560, loss = 0.00126588\n",
            "Iteration 561, loss = 0.00114822\n",
            "Iteration 562, loss = 0.00127969\n",
            "Iteration 563, loss = 0.00125085\n",
            "Iteration 564, loss = 0.00143050\n",
            "Iteration 565, loss = 0.00135236\n",
            "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(20, 100), max_iter=1000, tol=1e-05,\n",
              "              verbose=True)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rede_neural_credit.fit(x_credit_train, y_credit_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci2ZwIJB9VIj",
        "outputId": "b87d9daa-7245-484e-e5ec-2a50ed9c6cc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previsoes = rede_neural_credit.predict(x_credit_test)\n",
        "previsoes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "z1ylrIJ99ggH",
        "outputId": "c52ca1d2-7532-47f3-d6df-049b0055c335"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8b3ca473-2ba9-44e6-95e2-6362f0984148\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>default</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>408</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>677</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1360</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>813</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1900</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1524</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1767</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows Ã— 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b3ca473-2ba9-44e6-95e2-6362f0984148')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b3ca473-2ba9-44e6-95e2-6362f0984148 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b3ca473-2ba9-44e6-95e2-6362f0984148');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      default\n",
              "408         0\n",
              "1191        0\n",
              "677         0\n",
              "1360        1\n",
              "813         0\n",
              "...       ...\n",
              "1900        0\n",
              "1524        0\n",
              "1415        0\n",
              "1306        0\n",
              "1767        0\n",
              "\n",
              "[500 rows x 1 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_credit_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5QSypSAY9jhu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcvTO1IK9p6k",
        "outputId": "3968a5cb-cfbf-4580-a215-ee87e052b0d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.994"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_credit_test, previsoes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "065095dd569f083b06e963745b9e74965e6c5730dad4b2dff686b9902c737752"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
